# Steam Data Engineering Project

## PrÃ©sentation du projet

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre de lâ€™unitÃ© Data Engineering. Le principe du projet est de collecter des donnÃ©es depuis un site web grÃ¢ce en scrapant, stocker ces donnÃ©es dans une base de donnÃ©es, puis les exploiter Ã  travers une application web interactive et de pouvoir le faire tourner sur Docker.

Dans notre cas, nous avons choisi de scraper des donnÃ©es issues du site Steam, de stocker ces donnÃ©es dans une base MongoDB et Les afficher via une application web dÃ©veloppÃ©e avec Streamlit

## Scraping des donnÃ©es

Nous utilisons Scrapy pour :

Collecter les donnÃ©es des jeux Steam

Extraire nom, prix, rating, etc.

Structurer les donnÃ©es via items.py

Envoyer les donnÃ©es vers MongoDB via pipelines.py

Le scraping est dÃ©clenchÃ© automatiquement au lancement du container .


## Base de donnÃ©es

Les donnÃ©es collectÃ©es sont stockÃ©es dans une base MongoDB.

Nous utilisons MongoDB car il y'a une facilitÃ© dâ€™intÃ©gration avec Scrapy et que c'est compatible Docker

## Application Web

Lâ€™interface utilisateur a Ã©tÃ© dÃ©veloppÃ©e avec Streamlit.

Lâ€™application permet :

Dâ€™afficher les jeux scrapÃ©s

De visualiser les donnÃ©es sous forme de tableaux et de graphiques

Dâ€™explorer dynamiquement les informations stockÃ©es

nous avons choisi Streamlit pour ca simplicitÃ© d'utilisatio et sa rapiditÃ© de developpement

## Architecture Docker

Lâ€™ensemble du projet repose sur une architecture conteneurisÃ©e avec Docker et Docker Compose.

Le systÃ¨me est composÃ© de trois services :

MongoDB : base de donnÃ©es

Scraper : service chargÃ© de collecter les donnÃ©es

Web : application Streamlit permettant de visualiser les donnÃ©es

Lâ€™orchestration est assurÃ©e via un fichier docker-compose.yml, permettant de lancer lâ€™ensemble du projet en une seule commande.

# Lancement du projet

PrÃ©requis

Docker

Docker Compose

Ã‰tapes dâ€™exÃ©cution
git clone <url_du_repository>
cd <nom_du_projet>
docker compose up --build


Une fois les containers lancÃ©s, lâ€™application est accessible Ã  lâ€™adresse :

http://localhost:8501

Pour  acceder a notre projet il faut attendre que le terminal affiche tout ceci : steam_dashboard  |   Local URL: http://localhost:8501         
steam_dashboard  |   Network URL: http://172.18.0.4:8501      
steam_dashboard  |   External URL: http://90.79.144.99:8501   
steam_dashboard  |                                      
steam_scraper exited with code 0

# Choix techniques

Les choix technologiques ont Ã©tÃ© faits dans une logique de cohÃ©rence et de mise en pratique des concepts vus en cours :

Scrapy pour le scraping structurÃ©

MongoDB pour le stockage flexible des donnÃ©es

Streamlit pour la visualisation interactive

Docker & Docker Compose pour la gestion des services

## ğŸ“‚ Structure du projet

Lâ€™organisation du projet est la suivante :

```bash
PROJECT_DATA_ENGINEERING/
â”‚
â”œâ”€â”€ dashboard/               # Application web Streamlit
â”‚   â”œâ”€â”€ app.py               # Interface principale
â”‚   â”œâ”€â”€ db.py                # Connexion MongoDB
â”‚   â””â”€â”€ requirements.txt     # DÃ©pendances Python
â”‚
â”œâ”€â”€ steam_scraper/           # Projet Scrapy
â”‚   â”œâ”€â”€ spiders/             # Spiders de scraping
â”‚   â”‚   â””â”€â”€ steam_games.py
â”‚   â”‚
â”‚   â”œâ”€â”€ items.py             # Structure des donnÃ©es
â”‚   â”œâ”€â”€ pipelines.py         # Envoi vers MongoDB
â”‚   â”œâ”€â”€ settings.py          # Configuration Scrapy
â”‚
â”œâ”€â”€ docker-compose.yml       # Orchestration des services
â”œâ”€â”€ Dockerfile.scraper       # Image Docker Scraper
â”œâ”€â”€ Dockerfile.web           # Image Docker Web
â”œâ”€â”€ scrapy.cfg               # Configuration Scrapy
â””â”€â”€ README.md
```
